{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAN_cifar10.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPFr73pAJ65K8hSx5eBynmx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gianpDomiziani/TinyML/blob/main/GAN_cifar10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IR97wnkE2giB"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HRDTGnj2iLG"
      },
      "source": [
        "# GAN implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yepGaIU2RoK0"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XcUEkzygRukf",
        "outputId": "c3c2bf4e-8669-4bda-aa07-08a22f4a3bda"
      },
      "source": [
        "# To generate GIFs\r\n",
        "!pip install imageio\r\n",
        "!pip install git+https://github.com/tensorflow/docs"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: imageio in /usr/local/lib/python3.7/dist-packages (2.4.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from imageio) (7.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from imageio) (1.19.5)\n",
            "Collecting git+https://github.com/tensorflow/docs\n",
            "  Cloning https://github.com/tensorflow/docs to /tmp/pip-req-build-6a2yctu4\n",
            "  Running command git clone -q https://github.com/tensorflow/docs /tmp/pip-req-build-6a2yctu4\n",
            "Requirement already satisfied (use --upgrade to upgrade): tensorflow-docs===0.0.00c8dbd4ba403cf3fdd30917f86f817f5228d3812- from git+https://github.com/tensorflow/docs in /usr/local/lib/python3.7/dist-packages\n",
            "Requirement already satisfied: astor in /usr/local/lib/python3.7/dist-packages (from tensorflow-docs===0.0.00c8dbd4ba403cf3fdd30917f86f817f5228d3812-) (0.8.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from tensorflow-docs===0.0.00c8dbd4ba403cf3fdd30917f86f817f5228d3812-) (0.10.0)\n",
            "Requirement already satisfied: protobuf>=3.14 in /usr/local/lib/python3.7/dist-packages (from tensorflow-docs===0.0.00c8dbd4ba403cf3fdd30917f86f817f5228d3812-) (3.15.6)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from tensorflow-docs===0.0.00c8dbd4ba403cf3fdd30917f86f817f5228d3812-) (3.13)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py->tensorflow-docs===0.0.00c8dbd4ba403cf3fdd30917f86f817f5228d3812-) (1.15.0)\n",
            "Building wheels for collected packages: tensorflow-docs\n",
            "  Building wheel for tensorflow-docs (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tensorflow-docs: filename=tensorflow_docs-0.0.00c8dbd4ba403cf3fdd30917f86f817f5228d3812_-cp37-none-any.whl size=147330 sha256=432fe3f22ac7127469cbd1968038f99913e6e33984b250f0333e91416b9b482e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-x09xfl57/wheels/eb/1b/35/fce87697be00d2fc63e0b4b395b0d9c7e391a10e98d9a0d97f\n",
            "Successfully built tensorflow-docs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfXpCb4P2nYp"
      },
      "source": [
        "import keras\r\n",
        "from keras import layers\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "import glob\r\n",
        "import imageio\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import os\r\n",
        "import PIL\r\n",
        "import time\r\n",
        "\r\n",
        "from IPython import display"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVUJ1JQY4IOv"
      },
      "source": [
        "LATENT_DIM = 32\r\n",
        "HEIGHT = 32\r\n",
        "WIDTH = 32\r\n",
        "CHANNELS = 3"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06z3NrnS75Ap"
      },
      "source": [
        "# Build the generator model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aK_rjYPy4gcX",
        "outputId": "b480ffeb-b555-4a1c-9835-bead41bef63c"
      },
      "source": [
        "generator_input = keras.Input(shape=(LATENT_DIM,))\r\n",
        "\r\n",
        "x = layers.Dense(128 * 16 * 16)(generator_input)\r\n",
        "x = layers.BatchNormalization()(x)\r\n",
        "x = layers.LeakyReLU()(x)\r\n",
        "x = layers.Reshape((16, 16, 128))(x)  # Transform the input into a 16x16x128 feature map\r\n",
        "\r\n",
        "# CONV NET (without Pooling Layers)\r\n",
        "x = layers.Conv2D(256, 5, padding='same')(x)\r\n",
        "x = layers.BatchNormalization()(x)\r\n",
        "x = layers.LeakyReLU()(x)\r\n",
        "\r\n",
        "x = layers.Conv2DTranspose(256, 4, strides=2, padding='same')(x)  # Upsamples to 32x32\r\n",
        "x = layers.BatchNormalization()(x)\r\n",
        "x = layers.LeakyReLU()(x)\r\n",
        "\r\n",
        "x = layers.Conv2D(256, 5, padding='same')(x)\r\n",
        "x = layers.BatchNormalization()(x)\r\n",
        "x = layers.LeakyReLU()(x)\r\n",
        "x = layers.Conv2D(256, 5, padding='same')(x)\r\n",
        "x = layers.BatchNormalization()(x)\r\n",
        "x = layers.LeakyReLU()(x)\r\n",
        "\r\n",
        "x = layers.Conv2D(CHANNELS, 7, use_bias=False, activation='tanh', padding='same')(x)\r\n",
        "generator = keras.models.Model(generator_input, x)\r\n",
        "generator.summary()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 32)]              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 32768)             1048576   \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 32768)             131072    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu (LeakyReLU)      (None, 32768)             0         \n",
            "_________________________________________________________________\n",
            "reshape (Reshape)            (None, 16, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 16, 16, 256)       819200    \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 16, 16, 256)       1024      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)    (None, 16, 16, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose (Conv2DTran (None, 32, 32, 256)       1048576   \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 32, 32, 256)       1024      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)    (None, 32, 32, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 32, 32, 256)       1638400   \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 32, 32, 256)       1024      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)    (None, 32, 32, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 32, 32, 256)       1638400   \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 32, 32, 256)       1024      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)    (None, 32, 32, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 32, 32, 3)         37632     \n",
            "=================================================================\n",
            "Total params: 6,365,952\n",
            "Trainable params: 6,298,368\n",
            "Non-trainable params: 67,584\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "YCIFijyhRF4k",
        "outputId": "293fe2f4-dd04-4e36-8ddf-cd354f164b38"
      },
      "source": [
        "noise = tf.random.normal((1, LATENT_DIM))\r\n",
        "generated_image = generator(noise, training=False)\r\n",
        "\r\n",
        "plt.imshow(generated_image[0, :, :, 0], cmap='gray')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fcaa00f31d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WX5GHY2e8AC6"
      },
      "source": [
        "# Build the Discriminator model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z783ajsB8DsN",
        "outputId": "9a4671f7-f7af-45c1-aa11-50231b9ef39a"
      },
      "source": [
        "discriminator_input = layers.Input(shape=(HEIGHT, WIDTH, CHANNELS))\r\n",
        "x = layers.Conv2D(128, 3)(discriminator_input)\r\n",
        "x = layers.LeakyReLU()(x)\r\n",
        "x = layers.Conv2D(128, 4, strides=2)(x)\r\n",
        "x = layers.LeakyReLU()(x)\r\n",
        "x = layers.Dropout(0.3)(x)\r\n",
        "x = layers.Conv2D(128, 4, strides=2)(x)\r\n",
        "x = layers.LeakyReLU()(x)\r\n",
        "x = layers.Dropout(0.3)(x)\r\n",
        "x = layers.Conv2D(128, 4, strides=2)(x)\r\n",
        "x = layers.LeakyReLU()(x)\r\n",
        "x = layers.Flatten()(x)\r\n",
        "\r\n",
        "x = layers.Dropout(0.3)(x)\r\n",
        "\r\n",
        "x = layers.Dense(1, activation='sigmoid')(x)\r\n",
        "\r\n",
        "discriminator = keras.models.Model(discriminator_input, x)\r\n",
        "discriminator.summary()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 32, 32, 3)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 30, 30, 128)       3584      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_5 (LeakyReLU)    (None, 30, 30, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 14, 14, 128)       262272    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_6 (LeakyReLU)    (None, 14, 14, 128)       0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 14, 14, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 6, 6, 128)         262272    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_7 (LeakyReLU)    (None, 6, 6, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 6, 6, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 2, 2, 128)         262272    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_8 (LeakyReLU)    (None, 2, 2, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 513       \n",
            "=================================================================\n",
            "Total params: 790,913\n",
            "Trainable params: 790,913\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhQuvCB188dd"
      },
      "source": [
        "discriminator_optimizer = keras.optimizers.RMSprop(\r\n",
        "    learning_rate=0.0008,\r\n",
        "    clipvalue=1.0,\r\n",
        "    decay=1e-8\r\n",
        ")\r\n",
        "discriminator.compile(optimizer=discriminator_optimizer, loss='binary_crossentropy')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqcMgHDx9qmg"
      },
      "source": [
        "# Build the GAN network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahC7S14K9ugZ"
      },
      "source": [
        "discriminator.trainable = False  # set the discriminator weights to not trainable\r\n",
        "\r\n",
        "gan_input = keras.Input(shape=(LATENT_DIM,))\r\n",
        "gan_output = discriminator(generator(gan_input))\r\n",
        "gan = keras.models.Model(gan_input, gan_output)\r\n",
        "\r\n",
        "gan_optimizer = keras.optimizers.RMSprop(lr=0.0004, clipvalue=1.0, decay=1e-8)\r\n",
        "gan.compile(optimizer=gan_optimizer, loss='binary_crossentropy')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuKH_ddzBho1"
      },
      "source": [
        "# Implementing GAN training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRLbokVNBlj4"
      },
      "source": [
        "import os\r\n",
        "from keras.preprocessing import image"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdHhmpolBut9"
      },
      "source": [
        "(x_train, y_train), (_, _) = keras.datasets.cifar10.load_data()\r\n",
        "\r\n",
        "x_train = x_train[y_train.flatten() == 6]  # selects FROG images (class 6)\r\n",
        "\r\n",
        "x_train = x_train.reshape(\r\n",
        "    (x_train.shape[0],) +\r\n",
        "    (HEIGHT, WIDTH, CHANNELS)).astype('float32') / 255.  # normalizes data \r\n",
        "  \r\n",
        "ITERATIONS = 100000\r\n",
        "BATCH_SIZE = 256\r\n",
        "save_dir = './generated_images'"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WVTr1gfWEPN7",
        "outputId": "e0fcd043-1306-47c3-cc39-b859a5f7b6fc"
      },
      "source": [
        "start = 0\r\n",
        "for step in range(ITERATIONS):\r\n",
        "\r\n",
        "  random_latent_vectors = np.random.normal(size=(BATCH_SIZE, LATENT_DIM))  # samples random points in the latent space\r\n",
        "\r\n",
        "  generated_images = generator.predict(random_latent_vectors)  # decodes them to FAKE images\r\n",
        "\r\n",
        "  stop = start + BATCH_SIZE\r\n",
        "  real_images = x_train[start: stop]\r\n",
        "  combined_images = np.concatenate([generated_images, real_images])\r\n",
        "\r\n",
        "  labels = np.concatenate([np.zeros((BATCH_SIZE, 1)),\r\n",
        "                           np.ones((BATCH_SIZE, 1))])\r\n",
        "  \r\n",
        "  labels += 0.05 * np.random.random(labels.shape)  # adds random noise to the labels (hoping robusteness is reached)\r\n",
        "\r\n",
        "  d_loss = discriminator.train_on_batch(combined_images, labels)\r\n",
        "\r\n",
        "  random_latent_vectors = np.random.normal(size=(BATCH_SIZE, LATENT_DIM))\r\n",
        "\r\n",
        "  misleading_targets = np.ones((BATCH_SIZE, 1))  # assembles target for fooling the discriminator\r\n",
        "\r\n",
        "  a_loss = gan.train_on_batch(random_latent_vectors, misleading_targets)  # trains the generator (via the gan model, where the discriminator weights are frozen)\r\n",
        "\r\n",
        "  start += BATCH_SIZE\r\n",
        "  if start > len(x_train) - BATCH_SIZE:\r\n",
        "    start = 0\r\n",
        "\r\n",
        "  if step % 100 == 0:\r\n",
        "    gan.save_weights('gan.h5')\r\n",
        "    \r\n",
        "    print(f'STEP: {step}')\r\n",
        "    print('discriminator loss:', d_loss)\r\n",
        "    print('adversial loss:', a_loss)\r\n",
        "\r\n",
        "    img = image.array_to_img(generated_images[0] * 255., scale=False)\r\n",
        "    img.save(os.path.join(save_dir,\r\n",
        "                          'generated_frog' + str(step) + '.png'))\r\n",
        "    \r\n",
        "    img = image.array_to_img(real_images[0] * 255., scale=False)\r\n",
        "    img.save(os.path.join(save_dir,\r\n",
        "                          'real_frog' + str(step) + '.png'))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "STEP: 0\n",
            "discriminator loss: 0.7032589912414551\n",
            "adversial loss: 0.3455468714237213\n",
            "STEP: 100\n",
            "discriminator loss: 4.990224361419678\n",
            "adversial loss: 0.19758619368076324\n",
            "STEP: 200\n",
            "discriminator loss: -0.5396103858947754\n",
            "adversial loss: 3.0870583624571636e-29\n",
            "STEP: 300\n",
            "discriminator loss: -0.23083415627479553\n",
            "adversial loss: 2.7590572244662326e-06\n",
            "STEP: 400\n",
            "discriminator loss: 1.1199439764022827\n",
            "adversial loss: 1.5416007041931152\n",
            "STEP: 500\n",
            "discriminator loss: 0.7323792576789856\n",
            "adversial loss: 12.047283172607422\n",
            "STEP: 600\n",
            "discriminator loss: -1.613619327545166\n",
            "adversial loss: 0.0\n",
            "STEP: 700\n",
            "discriminator loss: -0.7193044424057007\n",
            "adversial loss: 8.857673645019531\n",
            "STEP: 800\n",
            "discriminator loss: -0.26037827134132385\n",
            "adversial loss: 3.867048740386963\n",
            "STEP: 900\n",
            "discriminator loss: 0.23838067054748535\n",
            "adversial loss: 4.11004114151001\n",
            "STEP: 1000\n",
            "discriminator loss: 0.5676189661026001\n",
            "adversial loss: 0.6532890796661377\n",
            "STEP: 1100\n",
            "discriminator loss: 0.6271795034408569\n",
            "adversial loss: 5.19265079498291\n",
            "STEP: 1200\n",
            "discriminator loss: 0.4345937669277191\n",
            "adversial loss: 0.869389533996582\n",
            "STEP: 1300\n",
            "discriminator loss: 0.7037867903709412\n",
            "adversial loss: 0.9721026420593262\n",
            "STEP: 1400\n",
            "discriminator loss: 0.6158833503723145\n",
            "adversial loss: 1.4999709129333496\n",
            "STEP: 1500\n",
            "discriminator loss: 0.4512827396392822\n",
            "adversial loss: 1.0035319328308105\n",
            "STEP: 1600\n",
            "discriminator loss: 0.6473212242126465\n",
            "adversial loss: 1.9589828252792358\n",
            "STEP: 1700\n",
            "discriminator loss: 0.5437670350074768\n",
            "adversial loss: 1.610869288444519\n",
            "STEP: 1800\n",
            "discriminator loss: 0.5283358097076416\n",
            "adversial loss: 0.5230044722557068\n",
            "STEP: 1900\n",
            "discriminator loss: 0.9986757636070251\n",
            "adversial loss: 0.8804838061332703\n",
            "STEP: 2000\n",
            "discriminator loss: 0.5552628636360168\n",
            "adversial loss: 1.4179801940917969\n",
            "STEP: 2100\n",
            "discriminator loss: 0.6929924488067627\n",
            "adversial loss: 1.0620849132537842\n",
            "STEP: 2200\n",
            "discriminator loss: 0.6593732237815857\n",
            "adversial loss: 1.1114764213562012\n",
            "STEP: 2300\n",
            "discriminator loss: 0.7995378375053406\n",
            "adversial loss: 2.1968350410461426\n",
            "STEP: 2400\n",
            "discriminator loss: 0.6674581170082092\n",
            "adversial loss: 0.6079020500183105\n",
            "STEP: 2500\n",
            "discriminator loss: 0.6313737630844116\n",
            "adversial loss: 0.8660902976989746\n",
            "STEP: 2600\n",
            "discriminator loss: 0.6005908846855164\n",
            "adversial loss: 0.8741949796676636\n",
            "STEP: 2700\n",
            "discriminator loss: 0.6339983940124512\n",
            "adversial loss: 0.8958417177200317\n",
            "STEP: 2800\n",
            "discriminator loss: 0.6595738530158997\n",
            "adversial loss: 0.6409564018249512\n",
            "STEP: 2900\n",
            "discriminator loss: 0.5869691371917725\n",
            "adversial loss: 1.6551635265350342\n",
            "STEP: 3000\n",
            "discriminator loss: 0.6728053689002991\n",
            "adversial loss: 0.758162260055542\n",
            "STEP: 3100\n",
            "discriminator loss: 0.6351586580276489\n",
            "adversial loss: 0.5681938529014587\n",
            "STEP: 3200\n",
            "discriminator loss: 0.648277223110199\n",
            "adversial loss: 1.2400712966918945\n",
            "STEP: 3300\n",
            "discriminator loss: 0.6800822615623474\n",
            "adversial loss: 0.8232510089874268\n",
            "STEP: 3400\n",
            "discriminator loss: 0.6642495393753052\n",
            "adversial loss: 0.7033147811889648\n",
            "STEP: 3500\n",
            "discriminator loss: 0.6598891019821167\n",
            "adversial loss: 1.075893759727478\n",
            "STEP: 3600\n",
            "discriminator loss: 0.6629540920257568\n",
            "adversial loss: 0.8948949575424194\n",
            "STEP: 3700\n",
            "discriminator loss: 0.6827529668807983\n",
            "adversial loss: 0.6748546361923218\n",
            "STEP: 3800\n",
            "discriminator loss: 0.6455666422843933\n",
            "adversial loss: 0.8029035329818726\n",
            "STEP: 3900\n",
            "discriminator loss: 0.6357216238975525\n",
            "adversial loss: 0.9848718047142029\n",
            "STEP: 4000\n",
            "discriminator loss: 0.67022305727005\n",
            "adversial loss: 0.8877294659614563\n",
            "STEP: 4100\n",
            "discriminator loss: 0.6376327276229858\n",
            "adversial loss: 1.001521348953247\n",
            "STEP: 4200\n",
            "discriminator loss: 0.7107903361320496\n",
            "adversial loss: 1.108595848083496\n",
            "STEP: 4300\n",
            "discriminator loss: 0.730550229549408\n",
            "adversial loss: 0.4264213442802429\n",
            "STEP: 4400\n",
            "discriminator loss: 0.6529136896133423\n",
            "adversial loss: 0.7363256216049194\n",
            "STEP: 4500\n",
            "discriminator loss: 0.8268736004829407\n",
            "adversial loss: 0.7503229975700378\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-3fd3b7e2707c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0mmisleading_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# assembles target for fooling the discriminator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m   \u001b[0ma_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_latent_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmisleading_targets\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# trains the generator (via the gan model, where the discriminator weights are frozen)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m   \u001b[0mstart\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[1;32m   1728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1729\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1730\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1731\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1732\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mreset_metrics\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1667\u001b[0m     \"\"\"\n\u001b[1;32m   1668\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1669\u001b[0;31m       \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m   def train_on_batch(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/metrics.py\u001b[0m in \u001b[0;36mreset_states\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[0mwhen\u001b[0m \u001b[0ma\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mevaluated\u001b[0m \u001b[0mduring\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m     \"\"\"\n\u001b[0;32m--> 253\u001b[0;31m     \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_set_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mabc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36mbatch_set_value\u001b[0;34m(tuples)\u001b[0m\n\u001b[1;32m   3704\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly_outside_functions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3705\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtuples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3706\u001b[0;31m       \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3707\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3708\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36massign\u001b[0;34m(self, value, use_locking, name, read_value)\u001b[0m\n\u001b[1;32m    891\u001b[0m             (tensor_name, self._shape, value_tensor.shape))\n\u001b[1;32m    892\u001b[0m       assign_op = gen_resource_variable_ops.assign_variable_op(\n\u001b[0;32m--> 893\u001b[0;31m           self.handle, value_tensor, name=name)\n\u001b[0m\u001b[1;32m    894\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mread_value\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lazy_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massign_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_resource_variable_ops.py\u001b[0m in \u001b[0;36massign_variable_op\u001b[0;34m(resource, value, name)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0;32m--> 142\u001b[0;31m         _ctx, \"AssignVariableOp\", name, resource, value)\n\u001b[0m\u001b[1;32m    143\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}